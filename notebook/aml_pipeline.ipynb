{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use partiiton strategy to solve larger scale route optimization problem. The rationale for partitioning is that usually an optimization problem could be hard to solve given the NP-hard nature for most of the optimization problems. To trade-off the result optimality and running time, one can partition the big problem into many smaller problems, then solve each smaller problem, and finially combine all results as the final result. The whole pipeline is illustrated by the below figure.\n",
    "\n",
    "<img src=../docs/media/pipeline.png width=\"90%\" />\n",
    "\n",
    "There are 4 main steps in the pipeline:\n",
    "1.  Reduce: It will try to assign some of the orders to truck routes in a heuristic way. The remaining unscheduled order will be passed to the later steps for optimization. This step is optional, namely, one can bypass this step but let optimizer search solution for all orders. However, reducing the search space by  heuristic can significantly reduce the search space. This will make it easier for the oprimization solver to find a good solution.  \n",
    "2.  Partition: This is core step to partition the big problem into smaller problems. \n",
    "3.  Solve: This step is to solve individual small problem using whatever optimization solver.\n",
    "4.  Merge: This final step is to combine all results from each small problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load libraries\n",
    "\n",
    "We use Azure ML pipeline for the implementation. Specifically, the partitioning step is done by the PrallelRunStep in Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output, load_component\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Setup some environment\n",
    "## 1.1.1 Load variables\n",
    "\n",
    "Some parameters are managed by environment variables.To specify your values, create a .env file in the root folder of the repository and set the values for the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ws_name = os.environ['AML_WORKSPACE_NAME']\n",
    "subscription_id = os.environ['AML_SUBSCRIPTION_ID']\n",
    "resource_group = os.environ['AML_RESOURCE_GROUP']\n",
    "\n",
    "\n",
    "print('---- Check Azure setting ----')\n",
    "print(f'AML Workspace name       : {ws_name}')\n",
    "print(f'Subscription ID          : {subscription_id}')\n",
    "print(f'Resource group           : {resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Azure authentication and Load Azure ML Workspace\n",
    "\n",
    "We are using DefaultAzureCredential to get access to workspace.\n",
    "\n",
    "DefaultAzureCredential should be capable of handling most Azure SDK authentication scenarios.\n",
    "\n",
    "Reference for more available credentials if it does not work for you: configure credential example, azure-identity reference doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": ws_name,\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Get Compute Cluster\n",
    "\n",
    "Read the compute name from the environment varibale. If it doest not exist in the Azure ML workspace, a new compute target will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# specify aml compute name for the optimization job\n",
    "cpu_compute_target = \"op-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"Standard_E4ds_v4\", min_instances=0, max_instances=10\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Create AML Environemnt and Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "env_name = 'op-env'\n",
    "\n",
    "try:\n",
    "    env = ml_client.environments.get(name=env_name, version=\"2\")\n",
    "    print(\"Found existing environment.\")\n",
    "\n",
    "except Exception as ex:\n",
    "    #Print the error message\n",
    "    print(ex)\n",
    "    \n",
    "    print(\"Creating new enviroment\")\n",
    "    env_docker_conda = Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "        conda_file=\"../src/env.yml\",\n",
    "        name=env_name,\n",
    "        description=\"Environment created from a Docker image plus Conda environment.\",\n",
    "    )\n",
    "    env = ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5 Prepare Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "order_path = \"../sample_data/order_large.csv\"\n",
    "distances_path = \"../sample_data/distance.csv\"\n",
    "# set the version number of the data asset\n",
    "v1 = \"initial\"\n",
    "\n",
    "order_data = Data(\n",
    "    name=\"order-data\",\n",
    "    version=v1,\n",
    "    description=\"Example order data\",\n",
    "    path=order_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "distances_data = Data(\n",
    "    name=\"distances-data\",\n",
    "    version=v1,\n",
    "    description=\"Example distance data\",\n",
    "    path=distances_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "## create order data asset if it doesn't already exist:\n",
    "try:\n",
    "    order_data_asset = ml_client.data.get(name=order_data.name, version=order_data.version)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {order_data.name}, version: {order_data.version}\"\n",
    "    )\n",
    "except:\n",
    "    ml_client.data.create_or_update(order_data)\n",
    "    print(f\"Data asset created. Name: {order_data.name}, version: {order_data.version}\")\n",
    "\n",
    "## create distances data asset if it doesn't already exist:\n",
    "try:\n",
    "    distances_data_asset = ml_client.data.get(name=distances_data.name, version=distances_data.version)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {distances_data.name}, version: {distances_data.version}\"\n",
    "    )\n",
    "except:\n",
    "    ml_client.data.create_or_update(distances_data)\n",
    "    print(f\"Data asset created. Name: {distances_data.name}, version: {distances_data.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Set up Azure ML Pipeline\n",
    "\n",
    "This section contains the main logic of the optimization pipeline.\n",
    "\n",
    "## 1.2.1 Reduce the search space of the problem\n",
    "\n",
    "The first step is to reduce the search space by assigning some of the orders based on heuristic. The detailed logic is implemented in the reduce.py. In general, if we use heuristic propoerly, we can achieve a good trade-off between result optimality and running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '../src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "reduce_component = command(\n",
    "    name=\"reduce_step\",\n",
    "    display_name=\"Reduce the problem space\",\n",
    "    description=\"Read the model input, create partial assignment by heuristic\",\n",
    "    inputs={\n",
    "        \"model_input\": Input(type=\"uri_file\"),\n",
    "        \"distance\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model_result_partial=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        model_input_reduced=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    command=\"\"\"python reduce.py \\\n",
    "            --model_input ${{inputs.model_input}} --distance ${{inputs.distance}} \\\n",
    "            --model_result_partial ${{outputs.model_result_partial}} --model_input_reduced ${{outputs.model_input_reduced}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{env.name}:{env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Partition the problem\n",
    "\n",
    "For large scale optimization problem, the problem space is just so big to solve practically. A commonly used idea is to partition the big problem into many smaller problems. Then solve each smaller problem individually and combine all the results as the final result. In some cases, the partition may not affect the result optimality, for example, in the route optimization problem, we can partition the orders by the delivery sources. In other cases, there will be trade-off between result optimality and running time when partitioning is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_component = command(\n",
    "    name=\"partition_step\",\n",
    "    display_name=\"Partition the big problem to many small problems\",\n",
    "    description=\"Read the reduced model input, partition the problem based on some heuristic\",\n",
    "    inputs={\n",
    "        \"model_input_reduced\": Input(type=\"uri_folder\"),\n",
    "        \"distance\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model_input_list=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    command=\"\"\"python partition.py \\\n",
    "            --model_input_reduced ${{inputs.model_input_reduced}} --distance ${{inputs.distance}} \\\n",
    "            --model_input_list ${{outputs.model_input_list}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{env.name}:{env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Solve individual problem\n",
    "\n",
    "After the problem is partitioned, we can solve each individul one by using whatever optimization solver. The optimization solver itself may leverage multi-process to speed up the search of result. This level of parallelism is totally controlled by the solver but not our Azure ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel task to process file data\n",
    "solve_component = parallel_run_function(\n",
    "    name=\"parallel_solver\",\n",
    "    display_name=\"Solve the small problems in parallel\",\n",
    "    description=\"parallel component for problem solver\",\n",
    "    inputs=dict(\n",
    "        model_input_list=Input(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=\"The data to be split and scored in parallel\",\n",
    "        ),\n",
    "        distance=Input(type=AssetTypes.URI_FOLDER, description='The distance file used by the solver.')\n",
    "    ),\n",
    "    outputs=dict(model_result_list=Output(type=AssetTypes.MLTABLE)),\n",
    "    input_data=\"${{inputs.model_input_list}}\",\n",
    "    instance_count=10,\n",
    "    max_concurrency_per_instance=1,\n",
    "    mini_batch_size=\"1\",\n",
    "    mini_batch_error_threshold=1,\n",
    "    retry_settings=dict(max_retries=2, timeout=60),\n",
    "    logging_level=\"DEBUG\",\n",
    "    task=RunFunction(\n",
    "        code=src_dir,\n",
    "        entry_script=\"solve.py\",\n",
    "        program_arguments=\"--distance ${{inputs.distance}}\",\n",
    "        environment=f\"{env.name}:{env.version}\",\n",
    "        append_row_to=\"${{outputs.model_result_list}}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Merge the results\n",
    "\n",
    "Once all the smaller problems are solved, we can combine the result as the final one. There could be chance to further optimize the result in this step in the case the previous partitioning will affect the global optimal. For example, one may combine two packages into the same truck from two seperated result if the combined one is more cost-efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge_component = command(\n",
    "    name=\"merge_step\",\n",
    "    display_name=\"Merge the result of the small problems\",\n",
    "    description=\"Merge the intermediate result as the final result\",\n",
    "    inputs={\n",
    "        \"model_input\": Input(type=\"uri_file\"),\n",
    "        \"distance\": Input(type=\"uri_file\"),\n",
    "        \"model_result_partial\": Input(type=\"uri_folder\"),\n",
    "        \"model_result_list\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model_result_final=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    command=\"\"\"python merge.py \\\n",
    "            --model_input ${{inputs.model_input}} --distance ${{inputs.distance}} \\\n",
    "            --model_result_partial ${{inputs.model_result_partial}} --model_result_list ${{inputs.model_result_list}} \\\n",
    "            --model_result_final ${{outputs.model_result_final}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{env.name}:{env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Run the Pipeline\n",
    "\n",
    "Finally, we chained all steps into a single Azure ML pipeline and submit it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target, \n",
    "    description=\"E2E route optimization pipeline\",\n",
    ")\n",
    "def route_optimization_pipeline(\n",
    "    pipeline_job_model_input,\n",
    "    pipeline_job_distance,\n",
    "):\n",
    "    # initialize the reduce step\n",
    "    reduce_job = reduce_component(\n",
    "        model_input=pipeline_job_model_input,\n",
    "        distance=pipeline_job_distance\n",
    "    )\n",
    "\n",
    "    # initialize the partition step\n",
    "    partition_job = partition_component(\n",
    "        model_input_reduced=reduce_job.outputs.model_input_reduced,  # note: using outputs from previous step\n",
    "        distance=pipeline_job_distance,  \n",
    "    )\n",
    "\n",
    "    # initialize the solve step\n",
    "    solve_job = solve_component(\n",
    "        model_input_list=partition_job.outputs.model_input_list,  # note: using outputs from previous step\n",
    "        distance=pipeline_job_distance,  \n",
    "    )\n",
    "\n",
    "    # initialize the merge step\n",
    "    merge_job = merge_component(\n",
    "        model_input=pipeline_job_model_input,  # note: using outputs from previous step\n",
    "        distance=pipeline_job_distance,  \n",
    "        model_result_partial=reduce_job.outputs.model_result_partial, # note: using outputs from previous step\n",
    "        model_result_list=solve_job.outputs.model_result_list, # note: using outputs from previous step\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    # pipeline_job_model_result_final\n",
    "    return {\n",
    "        \"pipeline_job_model_result_final\": merge_job.outputs.model_result_final,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = route_optimization_pipeline(\n",
    "    pipeline_job_model_input=Input(type=\"uri_file\", path=order_data_asset.id, mode=InputOutputModes.RO_MOUNT),\n",
    "    pipeline_job_distance=Input(type=\"uri_file\", path=distances_data_asset.id, mode=InputOutputModes.RO_MOUNT),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"route_optimization_demo\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Check the Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_schedule_path = '../tmp'\n",
    "\n",
    "# output = ml_client.jobs.download(name=pipeline_job.name, download_path=sample_schedule_path, all=True)\n",
    "# Download specific output\n",
    "output = ml_client.jobs.download(name=pipeline_job.name, download_path=sample_schedule_path, output_name='pipeline_job_model_result_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_output = pd.read_csv('../tmp/named-outputs/pipeline_job_model_result_final/schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimization-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

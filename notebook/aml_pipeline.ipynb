{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use partiiton strategy to solve larger scale route optimization problem. The rationale for partitioning is that usually an optimization problem could be hard to solve given the NP-hard nature for most of the optimization problems. To trade-off the result optimality and running time, one can partition the big problem into many smaller problems, then solve each smaller problem, and finially combine all results as the final result. The whole pipeline is illustrated by the below figure.\n",
    "\n",
    "<img src=../docs/media/pipeline.png width=\"90%\" />\n",
    "\n",
    "There are 4 main steps in the pipeline:\n",
    "1.  Reduce: It will try to assign some of the orders to truck routes in a heuristic way. The remaining unscheduled order will be passed to the later steps for optimization. This step is optional, namely, one can bypass this step but let optimizer search solution for all orders. However, reducing the search space by  heuristic can significantly reduce the search space. This will make it easier for the oprimization solver to find a good solution.  \n",
    "2.  Partition: This is core step to partition the big problem into smaller problems. \n",
    "3.  Solve: This step is to solve individual small problem using whatever optimization solver.\n",
    "4.  Merge: This final step is to combine all results from each small problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load libraries\n",
    "\n",
    "We use Azure ML pipeline for the implementation. Specifically, the partitioning step is done by the PrallelRunStep in Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output, load_component\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Setup some environment\n",
    "## 1.1.1 Load variables\n",
    "\n",
    "Some parameters are managed by environment variables.To specify your values, create a .env file in the root folder of the repository and set the values for the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Check Azure setting ----\n",
      "AML Workspace name       : amldemo\n",
      "Subscription ID          : e4eda206-7aff-4e54-8f55-1e60f0e64093\n",
      "Resource group           : aml-demo-rg\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ws_name = os.environ['AML_WORKSPACE_NAME']\n",
    "subscription_id = os.environ['AML_SUBSCRIPTION_ID']\n",
    "resource_group = os.environ['AML_RESOURCE_GROUP']\n",
    "\n",
    "\n",
    "print('---- Check Azure setting ----')\n",
    "print(f'AML Workspace name       : {ws_name}')\n",
    "print(f'Subscription ID          : {subscription_id}')\n",
    "print(f'Resource group           : {resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Azure authentication and Load Azure ML Workspace\n",
    "\n",
    "We are using DefaultAzureCredential to get access to workspace.\n",
    "\n",
    "DefaultAzureCredential should be capable of handling most Azure SDK authentication scenarios.\n",
    "\n",
    "Reference for more available credentials if it does not work for you: configure credential example, azure-identity reference doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\zhianhe\\Demo\\.azureml\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x000002A26D6C78E0>,\n",
      "         subscription_id=e4eda206-7aff-4e54-8f55-1e60f0e64093,\n",
      "         resource_group_name=aml-demo-rg,\n",
      "         workspace_name=amldemo)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": ws_name,\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Get Compute Cluster\n",
    "\n",
    "Read the compute name from the environment varibale. If it doest not exist in the Azure ML workspace, a new compute target will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# specify aml compute name for the optimization job\n",
    "cpu_compute_target = \"op-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"Standard_E4ds_v4\", min_instances=0, max_instances=10\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Create AML Environemnt and Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing environment.\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "env_name = 'op-env'\n",
    "\n",
    "try:\n",
    "    env = ml_client.environments.get(name=env_name, version=\"1\")\n",
    "    print(\"Found existing environment.\")\n",
    "\n",
    "except Exception as ex:\n",
    "    #Print the error message\n",
    "    print(ex)\n",
    "    \n",
    "    print(\"Creating new enviroment\")\n",
    "    env_docker_conda = Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "        conda_file=\"../src/env.yml\",\n",
    "        name=env_name,\n",
    "        description=\"Environment created from a Docker image plus Conda environment.\",\n",
    "    )\n",
    "    env = ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5 Prepare Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asset already exists. Name: order-data, version: initial\n",
      "Data asset already exists. Name: distances-data, version: initial\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "order_path = \"../sample_data/order_large.csv\"\n",
    "distances_path = \"../sample_data/distance.csv\"\n",
    "# set the version number of the data asset\n",
    "v1 = \"initial\"\n",
    "\n",
    "order_data = Data(\n",
    "    name=\"order-data\",\n",
    "    version=v1,\n",
    "    description=\"Example order data\",\n",
    "    path=order_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "distances_data = Data(\n",
    "    name=\"distances-data\",\n",
    "    version=v1,\n",
    "    description=\"Example distance data\",\n",
    "    path=distances_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "## create order data asset if it doesn't already exist:\n",
    "try:\n",
    "    order_data_asset = ml_client.data.get(name=order_data.name, version=order_data.version)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {order_data.name}, version: {order_data.version}\"\n",
    "    )\n",
    "except:\n",
    "    ml_client.data.create_or_update(order_data)\n",
    "    print(f\"Data asset created. Name: {order_data.name}, version: {order_data.version}\")\n",
    "\n",
    "## create distances data asset if it doesn't already exist:\n",
    "try:\n",
    "    distances_data_asset = ml_client.data.get(name=distances_data.name, version=distances_data.version)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {distances_data.name}, version: {distances_data.version}\"\n",
    "    )\n",
    "except:\n",
    "    ml_client.data.create_or_update(distances_data)\n",
    "    print(f\"Data asset created. Name: {distances_data.name}, version: {distances_data.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Set up Azure ML Pipeline\n",
    "\n",
    "This section contains the main logic of the optimization pipeline.\n",
    "\n",
    "## 1.2.1 Reduce the search space of the problem\n",
    "\n",
    "The first step is to reduce the search space by assigning some of the orders based on heuristic. The detailed logic is implemented in the reduce.py. In general, if we use heuristic propoerly, we can achieve a good trade-off between result optimality and running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the components as functions\n",
    "from src.reduce import reduce_component\n",
    "\n",
    "cluster_name = \"cpu_compute_target\"\n",
    "# define a pipeline with component\n",
    "@pipeline(default_compute=cluster_name)\n",
    "def pipeline_with_python_function_components(model_input, distance):\n",
    "    \"\"\"E2E dummy train-score-eval pipeline with components defined via Python function components\"\"\"\n",
    "\n",
    "    # Call component obj as function: apply given inputs & parameters to create a node in pipeline\n",
    "    reduce_step = reduce_component(\n",
    "        model_input=model_input, distance=distance, learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # Return: pipeline outputs\n",
    "    return {\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_job = pipeline_with_python_function_components(\n",
    "    input_data=Input(\n",
    "        path=\"wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\", type=\"uri_file\"\n",
    "    ),\n",
    "    test_data=Input(\n",
    "        path=\"wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\", type=\"uri_file\"\n",
    "    ),\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"train_score_eval_pipeline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Partition the problem\n",
    "\n",
    "For large scale optimization problem, the problem space is just so big to solve practically. A commonly used idea is to partition the big problem into many smaller problems. Then solve each smaller problem individually and combine all the results as the final result. In some cases, the partition may not affect the result optimality, for example, in the route optimization problem, we can partition the orders by the delivery sources. In other cases, there will be trade-off between result optimality and running time when partitioning is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "model_input_list = PipelineData(\"model_input_list\",datastore=def_blob_store).as_dataset()\n",
    "\n",
    "parition_step = PythonScriptStep(\n",
    "    script_name=\"partition.py\", \n",
    "    arguments=[\"--model_input_reduced\", model_input_reduced,\n",
    "                \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "                \"--model_input_list\", model_input_list],\n",
    "    inputs=[model_input_reduced],\n",
    "    outputs=[model_input_list],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Solve individual problem\n",
    "\n",
    "After the problem is partitioned, we can solve each individul one by using whatever optimization solver. The optimization solver itself may leverage multi-process to speed up the search of result. This level of parallelism is totally controlled by the solver but not our Azure ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_list = PipelineData(\"model_result_list\", datastore=def_blob_store)\n",
    "\n",
    "# pass distance file as side input\n",
    "local_path = \"/tmp/{}\".format(str(uuid.uuid4()))\n",
    "distance_config = distance.as_named_input(\"distance\").as_mount(local_path)\n",
    "\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script='solve.py',\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=-1,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"model_result_list.txt\",\n",
    "    environment=env,\n",
    "    compute_target=aml_compute,\n",
    "    run_invocation_timeout=600,\n",
    "    node_count=max_nodes)\n",
    "\n",
    "solve_step = ParallelRunStep(\n",
    "    name=\"solve\",\n",
    "    inputs=[model_input_list.as_named_input('model_input_list')],\n",
    "    output=model_result_list,\n",
    "    arguments=[\"--distance\", distance_config],\n",
    "    side_inputs=[distance_config],\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Merge the results\n",
    "\n",
    "Once all the smaller problems are solved, we can combine the result as the final one. There could be chance to further optimize the result in this step in the case the previous partitioning will affect the global optimal. For example, one may combine two packages into the same truck from two seperated result if the combined one is more cost-efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace it with the path you uploaded to Azure ML default datestore.\n",
    "model_output_path = 'model_output'\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_final = OutputFileDatasetConfig(destination=(def_blob_store, model_output_path))\n",
    "\n",
    "merge_step = PythonScriptStep(\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--model_input\", model_input.as_named_input('model_input').as_download(path_on_compute='order_file'), \n",
    "    \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "    \"--model_result_partial\", model_result_partial, \n",
    "    \"--model_result_list\", model_result_list, \n",
    "    \"--model_result_final\", model_result_final],\n",
    "    inputs=[model_result_partial, model_result_list],\n",
    "    outputs=[model_result_final],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Run the Pipeline\n",
    "\n",
    "Finally, we chained all steps into a single Azure ML pipeline and submit it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step, parition_step, solve_step, merge_step])\n",
    "\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "# RunDetails(pipeline_run).show()\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Check the Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = Dataset.Tabular.from_delimited_files(path=(def_blob_store, model_output_path))\n",
    "\n",
    "print(model_output.to_pandas_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Publish the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the pipeline to Azure ML \n",
    "\n",
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='Route Optimization', description='Demo for route optimization', version='1.0')\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the endpoint of the pipeline\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

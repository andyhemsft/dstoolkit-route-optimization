{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use partiiton strategy to solve larger scale route optimization problem. The rationale for partitioning is that usually an optimization problem could be hard to solve given the NP-hard nature for most of the optimization problems. To trade-off the result optimality and running time, one can partition the big problem into many smaller problems, then solve each smaller problem, and finially combine all results as the final result. The whole pipeline is illustrated by the below figure.\n",
    "\n",
    "<img src=../docs/media/pipeline.png width=\"90%\" />\n",
    "\n",
    "There are 4 main steps in the pipeline:\n",
    "1.  Reduce: It will try to assign some of the orders to truck routes in a heuristic way. The remaining unscheduled order will be passed to the later steps for optimization. This step is optional, namely, one can bypass this step but let optimizer search solution for all orders. However, reducing the search space by  heuristic can significantly reduce the search space. This will make it easier for the oprimization solver to find a good solution.  \n",
    "2.  Partition: This is core step to partition the big problem into smaller problems. \n",
    "3.  Solve: This step is to solve individual small problem using whatever optimization solver.\n",
    "4.  Merge: This final step is to combine all results from each small problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load libraries\n",
    "\n",
    "We use Azure ML pipeline for the implementation. Specifically, the partitioning step is done by the PrallelRunStep in Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output, load_component\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Setup some environment\n",
    "## 1.1.1 Load variables\n",
    "\n",
    "Some parameters are managed by environment variables.To specify your values, create a .env file in the root folder of the repository and set the values for the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Check Azure setting ----\n",
      "AML Workspace name       : amldemo\n",
      "Subscription ID          : e4eda206-7aff-4e54-8f55-1e60f0e64093\n",
      "Resource group           : aml-demo-rg\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ws_name = os.environ['AML_WORKSPACE_NAME']\n",
    "subscription_id = os.environ['AML_SUBSCRIPTION_ID']\n",
    "resource_group = os.environ['AML_RESOURCE_GROUP']\n",
    "\n",
    "\n",
    "print('---- Check Azure setting ----')\n",
    "print(f'AML Workspace name       : {ws_name}')\n",
    "print(f'Subscription ID          : {subscription_id}')\n",
    "print(f'Resource group           : {resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Azure authentication and Load Azure ML Workspace\n",
    "\n",
    "We are using DefaultAzureCredential to get access to workspace.\n",
    "\n",
    "DefaultAzureCredential should be capable of handling most Azure SDK authentication scenarios.\n",
    "\n",
    "Reference for more available credentials if it does not work for you: configure credential example, azure-identity reference doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\zhianhe\\Demo\\.azureml\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x000001EDBC53BD90>,\n",
      "         subscription_id=e4eda206-7aff-4e54-8f55-1e60f0e64093,\n",
      "         resource_group_name=aml-demo-rg,\n",
      "         workspace_name=amldemo)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ml_client = MLClient.from_config(credential=credential)\n",
    "except Exception as ex:\n",
    "    # NOTE: Update following workspace information if not correctly configure before\n",
    "    client_config = {\n",
    "        \"subscription_id\": subscription_id,\n",
    "        \"resource_group\": resource_group,\n",
    "        \"workspace_name\": ws_name,\n",
    "    }\n",
    "\n",
    "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
    "        print(\n",
    "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
    "        )\n",
    "        raise ex\n",
    "    else:  # write and reload from config file\n",
    "        import json, os\n",
    "\n",
    "        config_path = \"../.azureml/config.json\"\n",
    "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "        with open(config_path, \"w\") as fo:\n",
    "            fo.write(json.dumps(client_config))\n",
    "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
    "print(ml_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Get Compute Cluster\n",
    "\n",
    "Read the compute name from the environment varibale. If it doest not exist in the Azure ML workspace, a new compute target will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "# specify aml compute name for the optimization job\n",
    "cpu_compute_target = \"op-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"Standard_E4ds_v4\", min_instances=0, max_instances=10\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Create AML Environemnt and Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing environment.\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "env_name = 'op-env'\n",
    "\n",
    "try:\n",
    "    env = ml_client.environments.get(name=env_name, version=\"2\")\n",
    "    print(\"Found existing environment.\")\n",
    "\n",
    "except Exception as ex:\n",
    "    #Print the error message\n",
    "    print(ex)\n",
    "    \n",
    "    print(\"Creating new enviroment\")\n",
    "    env_docker_conda = Environment(\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "        conda_file=\"../src/env.yml\",\n",
    "        name=env_name,\n",
    "        description=\"Environment created from a Docker image plus Conda environment.\",\n",
    "    )\n",
    "    env = ml_client.environments.create_or_update(env_docker_conda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5 Prepare Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading order_large.csv\u001b[32m (< 1 MB): 100%|##########| 653k/653k [00:00<00:00, 836kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asset created. Name: orders, version: 1\n",
      "Data asset created. Name: distances-matrix, version: 1\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "order_path = \"../sample_data/order_large.csv\"\n",
    "distances_path = \"../sample_data/distance.csv\"\n",
    "# set the version number of the data asset\n",
    "v1 = \"1\"\n",
    "\n",
    "order_data = Data(\n",
    "    name=\"orders\",\n",
    "    version=v1,\n",
    "    description=\"Example order data\",\n",
    "    path=order_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "distances_data = Data(\n",
    "    name=\"distances-matrix\",\n",
    "    version=v1,\n",
    "    description=\"Example distance data\",\n",
    "    path=distances_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    ")\n",
    "\n",
    "## create order data asset if it doesn't already exist:\n",
    "try:\n",
    "    order_data_asset = ml_client.data.get(name=order_data.name, version=order_data.version)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {order_data.name}, version: {order_data.version}\"\n",
    "    )\n",
    "except:\n",
    "    ml_client.data.create_or_update(order_data)\n",
    "    order_data_asset = ml_client.data.get(name=order_data.name, version=order_data.version)\n",
    "    print(f\"Data asset created. Name: {order_data.name}, version: {order_data.version}\")\n",
    "\n",
    "## create distances data asset if it doesn't already exist:\n",
    "try:\n",
    "    distances_data_asset = ml_client.data.get(name=distances_data.name, version=distances_data.version)\n",
    "    print(\n",
    "        f\"Data asset already exists. Name: {distances_data.name}, version: {distances_data.version}\"\n",
    "    )\n",
    "except:\n",
    "    ml_client.data.create_or_update(distances_data)\n",
    "    distances_data_asset = ml_client.data.get(name=distances_data.name, version=distances_data.version)\n",
    "    print(f\"Data asset created. Name: {distances_data.name}, version: {distances_data.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Set up Azure ML Pipeline\n",
    "\n",
    "This section contains the main logic of the optimization pipeline.\n",
    "\n",
    "## 1.2.1 Reduce the search space of the problem\n",
    "\n",
    "The first step is to reduce the search space by assigning some of the orders based on heuristic. The detailed logic is implemented in the reduce.py. In general, if we use heuristic propoerly, we can achieve a good trade-off between result optimality and running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '../src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "reduce_component = command(\n",
    "    name=\"reduce_step\",\n",
    "    display_name=\"Reduce the problem space\",\n",
    "    description=\"Read the model input, create partial assignment by heuristic\",\n",
    "    inputs={\n",
    "        \"model_input\": Input(type=\"uri_file\"),\n",
    "        \"distance\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model_result_partial=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        model_input_reduced=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    command=\"\"\"python reduce.py \\\n",
    "            --model_input ${{inputs.model_input}} --distance ${{inputs.distance}} \\\n",
    "            --model_result_partial ${{outputs.model_result_partial}} --model_input_reduced ${{outputs.model_input_reduced}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{env.name}:{env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Partition the problem\n",
    "\n",
    "For large scale optimization problem, the problem space is just so big to solve practically. A commonly used idea is to partition the big problem into many smaller problems. Then solve each smaller problem individually and combine all the results as the final result. In some cases, the partition may not affect the result optimality, for example, in the route optimization problem, we can partition the orders by the delivery sources. In other cases, there will be trade-off between result optimality and running time when partitioning is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_component = command(\n",
    "    name=\"partition_step\",\n",
    "    display_name=\"Partition the big problem to many small problems\",\n",
    "    description=\"Read the reduced model input, partition the problem based on some heuristic\",\n",
    "    inputs={\n",
    "        \"model_input_reduced\": Input(type=\"uri_folder\"),\n",
    "        \"distance\": Input(type=\"uri_file\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model_input_list=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    command=\"\"\"python partition.py \\\n",
    "            --model_input_reduced ${{inputs.model_input_reduced}} --distance ${{inputs.distance}} \\\n",
    "            --model_input_list ${{outputs.model_input_list}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{env.name}:{env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Solve individual problem\n",
    "\n",
    "After the problem is partitioned, we can solve each individul one by using whatever optimization solver. The optimization solver itself may leverage multi-process to speed up the search of result. This level of parallelism is totally controlled by the solver but not our Azure ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel task to process file data\n",
    "solve_component = parallel_run_function(\n",
    "    name=\"parallel_solver\",\n",
    "    display_name=\"Solve the small problems in parallel\",\n",
    "    description=\"parallel component for problem solver\",\n",
    "    inputs=dict(\n",
    "        model_input_list=Input(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=\"The data to be split and scored in parallel\",\n",
    "        ),\n",
    "        distance=Input(type=AssetTypes.URI_FOLDER, description='The distance file used by the solver.')\n",
    "    ),\n",
    "    outputs=dict(model_result_list=Output(type=AssetTypes.MLTABLE)),\n",
    "    input_data=\"${{inputs.model_input_list}}\",\n",
    "    instance_count=10,\n",
    "    max_concurrency_per_instance=1,\n",
    "    mini_batch_size=\"1\",\n",
    "    mini_batch_error_threshold=1,\n",
    "    retry_settings=dict(max_retries=2, timeout=240), # make sure the timeout is larger than the timeout of the task\n",
    "    logging_level=\"DEBUG\",\n",
    "    task=RunFunction(\n",
    "        code=src_dir,\n",
    "        entry_script=\"solve.py\",\n",
    "        program_arguments=\"--distance ${{inputs.distance}}\",\n",
    "        environment=f\"{env.name}:{env.version}\",\n",
    "        append_row_to=\"${{outputs.model_result_list}}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Merge the results\n",
    "\n",
    "Once all the smaller problems are solved, we can combine the result as the final one. There could be chance to further optimize the result in this step in the case the previous partitioning will affect the global optimal. For example, one may combine two packages into the same truck from two seperated result if the combined one is more cost-efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merge_component = command(\n",
    "    name=\"merge_step\",\n",
    "    display_name=\"Merge the result of the small problems\",\n",
    "    description=\"Merge the intermediate result as the final result\",\n",
    "    inputs={\n",
    "        \"model_input\": Input(type=\"uri_file\"),\n",
    "        \"distance\": Input(type=\"uri_file\"),\n",
    "        \"model_result_partial\": Input(type=\"uri_folder\"),\n",
    "        \"model_result_list\": Input(type=\"uri_folder\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model_result_final=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=src_dir,\n",
    "    command=\"\"\"python merge.py \\\n",
    "            --model_input ${{inputs.model_input}} --distance ${{inputs.distance}} \\\n",
    "            --model_result_partial ${{inputs.model_result_partial}} --model_result_list ${{inputs.model_result_list}} \\\n",
    "            --model_result_final ${{outputs.model_result_final}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{env.name}:{env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Run the Pipeline\n",
    "\n",
    "Finally, we chained all steps into a single Azure ML pipeline and submit it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target, \n",
    "    description=\"E2E route optimization pipeline\",\n",
    ")\n",
    "def route_optimization_pipeline(\n",
    "    pipeline_job_model_input,\n",
    "    pipeline_job_distance,\n",
    "):\n",
    "    # initialize the reduce step\n",
    "    reduce_job = reduce_component(\n",
    "        model_input=pipeline_job_model_input,\n",
    "        distance=pipeline_job_distance\n",
    "    )\n",
    "\n",
    "    # initialize the partition step\n",
    "    partition_job = partition_component(\n",
    "        model_input_reduced=reduce_job.outputs.model_input_reduced,  # note: using outputs from previous step\n",
    "        distance=pipeline_job_distance,  \n",
    "    )\n",
    "\n",
    "    # initialize the solve step\n",
    "    solve_job = solve_component(\n",
    "        model_input_list=partition_job.outputs.model_input_list,  # note: using outputs from previous step\n",
    "        distance=pipeline_job_distance,  \n",
    "    )\n",
    "\n",
    "    # initialize the merge step\n",
    "    merge_job = merge_component(\n",
    "        model_input=pipeline_job_model_input,  # note: using outputs from previous step\n",
    "        distance=pipeline_job_distance,  \n",
    "        model_result_partial=reduce_job.outputs.model_result_partial, # note: using outputs from previous step\n",
    "        model_result_list=solve_job.outputs.model_result_list, # note: using outputs from previous step\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    # pipeline_job_model_result_final\n",
    "    return {\n",
    "        \"pipeline_job_model_result_final\": merge_job.outputs.model_result_final,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = route_optimization_pipeline(\n",
    "    pipeline_job_model_input=Input(type=\"uri_file\", path=order_data_asset.id, mode=InputOutputModes.RO_MOUNT),\n",
    "    pipeline_job_distance=Input(type=\"uri_file\", path=distances_data_asset.id, mode=InputOutputModes.RO_MOUNT),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.MLTableJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: olive_pasta_sym445v2ny\n",
      "Web View: https://ml.azure.com/runs/olive_pasta_sym445v2ny?wsid=/subscriptions/e4eda206-7aff-4e54-8f55-1e60f0e64093/resourcegroups/aml-demo-rg/workspaces/amldemo\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2025-01-31 04:23:43Z] Submitting 1 runs, first five are: 85c5104d:bb669507-09bc-4981-817d-d91d14f0c93b\n",
      "[2025-01-31 04:28:24Z] Completing processing run id bb669507-09bc-4981-817d-d91d14f0c93b.\n",
      "[2025-01-31 04:28:25Z] Submitting 1 runs, first five are: cea3f411:7aed87c9-d9ca-41e0-a0b2-9e3ad20cf663\n",
      "[2025-01-31 04:29:07Z] Completing processing run id 7aed87c9-d9ca-41e0-a0b2-9e3ad20cf663.\n",
      "[2025-01-31 04:29:08Z] Submitting 1 runs, first five are: e7030575:5da5e3bf-d676-42e0-9a8e-ff567ab67f85\n",
      "[2025-01-31 04:47:22Z] Execution of experiment failed, update experiment status and cancel running nodes.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: olive_pasta_sym445v2ny\n",
      "Web View: https://ml.azure.com/runs/olive_pasta_sym445v2ny?wsid=/subscriptions/e4eda206-7aff-4e54-8f55-1e60f0e64093/resourcegroups/aml-demo-rg/workspaces/amldemo\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /parallel_solver. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"southeastasia\",\n    \"location\": \"southeastasia\",\n    \"time\": \"2025-01-31T04:47:22.568872Z\",\n    \"component_name\": \"\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Force the pipeline to rerun all steps\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pipeline_job\u001b[38;5;241m.\u001b[39mforce_rerun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhianhe\\AppData\\Local\\miniconda3\\envs\\route-optimization-v2\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\zhianhe\\AppData\\Local\\miniconda3\\envs\\route-optimization-v2\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[0;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[0;32m    288\u001b[0m         ):\n\u001b[1;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "File \u001b[1;32mc:\\Users\\zhianhe\\AppData\\Local\\miniconda3\\envs\\route-optimization-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:858\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m--> 858\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhianhe\\AppData\\Local\\miniconda3\\envs\\route-optimization-v2\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_ops_helper.py:334\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[1;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[0;32m    332\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[0;32m    335\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[0;32m    336\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[0;32m    337\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    338\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[0;32m    339\u001b[0m         )\n\u001b[0;32m    341\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    342\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. Failed nodes: /parallel_solver. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"message_format\": \"Pipeline has failed child jobs. {0}\",\n        \"message_parameters\": {},\n        \"reference_code\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"southeastasia\",\n    \"location\": \"southeastasia\",\n    \"time\": \"2025-01-31T04:47:22.568872Z\",\n    \"component_name\": \"\"\n} "
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"route_optimization_demo\",\n",
    ")\n",
    "\n",
    "# Force the pipeline to rerun all steps\n",
    "pipeline_job.force_rerun = True\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Check the Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifact azureml://subscriptions/e4eda206-7aff-4e54-8f55-1e60f0e64093/resourcegroups/aml-demo-rg/workspaces/amldemo/datastores/workspaceblobstore/paths/azureml/1cb38885-7704-492b-ba20-3113e85b2d3b/model_result_final/ to ..\\tmp\\named-outputs\\pipeline_job_model_result_final\n"
     ]
    }
   ],
   "source": [
    "sample_schedule_path = '../tmp'\n",
    "\n",
    "# output = ml_client.jobs.download(name=pipeline_job.name, download_path=sample_schedule_path, all=True)\n",
    "# Download specific output\n",
    "output = ml_client.jobs.download(name=pipeline_job.name, download_path=sample_schedule_path, output_name='pipeline_job_model_result_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_output = pd.read_csv('../tmp/named-outputs/pipeline_job_model_result_final/schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Schedule_ID</th>\n",
       "      <th>Truck_Route</th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Material_ID</th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Danger_Type</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Deadline</th>\n",
       "      <th>Shared_Truck</th>\n",
       "      <th>Truck_Type</th>\n",
       "      <th>Area_Rate</th>\n",
       "      <th>Weight_Rate</th>\n",
       "      <th>Capacity_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0157f3db-2a5b-4738-865a-bdec22d1b043</td>\n",
       "      <td>City_61-&gt;City_19</td>\n",
       "      <td>A230153</td>\n",
       "      <td>B-6298</td>\n",
       "      <td>P01-70d4c91f-65e7-434c-89dc-5653fa3d4dff</td>\n",
       "      <td>type_1</td>\n",
       "      <td>City_61</td>\n",
       "      <td>City_19</td>\n",
       "      <td>2022-04-05 23:59:59</td>\n",
       "      <td>2022-04-07 01:21:40</td>\n",
       "      <td>2022-04-07 11:59:59</td>\n",
       "      <td>N</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.880099</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.954333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0157f3db-2a5b-4738-865a-bdec22d1b043</td>\n",
       "      <td>City_61-&gt;City_19</td>\n",
       "      <td>A230153</td>\n",
       "      <td>B-6298</td>\n",
       "      <td>P01-291a1abe-8b52-4fa9-9667-f6aaba4238bb</td>\n",
       "      <td>type_1</td>\n",
       "      <td>City_61</td>\n",
       "      <td>City_19</td>\n",
       "      <td>2022-04-05 23:59:59</td>\n",
       "      <td>2022-04-07 01:21:40</td>\n",
       "      <td>2022-04-07 11:59:59</td>\n",
       "      <td>N</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.880099</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.954333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0157f3db-2a5b-4738-865a-bdec22d1b043</td>\n",
       "      <td>City_61-&gt;City_19</td>\n",
       "      <td>A230153</td>\n",
       "      <td>B-6298</td>\n",
       "      <td>P01-1dc8de6a-7ad4-4a3b-a5aa-4b536349f3d7</td>\n",
       "      <td>type_1</td>\n",
       "      <td>City_61</td>\n",
       "      <td>City_19</td>\n",
       "      <td>2022-04-05 23:59:59</td>\n",
       "      <td>2022-04-07 01:21:40</td>\n",
       "      <td>2022-04-07 11:59:59</td>\n",
       "      <td>N</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.880099</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.954333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0157f3db-2a5b-4738-865a-bdec22d1b043</td>\n",
       "      <td>City_61-&gt;City_19</td>\n",
       "      <td>A230153</td>\n",
       "      <td>B-6298</td>\n",
       "      <td>P01-e1c4511f-f1fc-4f89-b3bd-dea13841510c</td>\n",
       "      <td>type_1</td>\n",
       "      <td>City_61</td>\n",
       "      <td>City_19</td>\n",
       "      <td>2022-04-05 23:59:59</td>\n",
       "      <td>2022-04-07 01:21:40</td>\n",
       "      <td>2022-04-07 11:59:59</td>\n",
       "      <td>N</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.880099</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.954333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0157f3db-2a5b-4738-865a-bdec22d1b043</td>\n",
       "      <td>City_61-&gt;City_19</td>\n",
       "      <td>A230153</td>\n",
       "      <td>B-6298</td>\n",
       "      <td>P01-86e9b1e3-8f73-49f0-9573-a52a92bbd0f6</td>\n",
       "      <td>type_1</td>\n",
       "      <td>City_61</td>\n",
       "      <td>City_19</td>\n",
       "      <td>2022-04-05 23:59:59</td>\n",
       "      <td>2022-04-07 01:21:40</td>\n",
       "      <td>2022-04-07 11:59:59</td>\n",
       "      <td>N</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.880099</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.954333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Schedule_ID       Truck_Route Order_ID  \\\n",
       "0  0157f3db-2a5b-4738-865a-bdec22d1b043  City_61->City_19  A230153   \n",
       "1  0157f3db-2a5b-4738-865a-bdec22d1b043  City_61->City_19  A230153   \n",
       "2  0157f3db-2a5b-4738-865a-bdec22d1b043  City_61->City_19  A230153   \n",
       "3  0157f3db-2a5b-4738-865a-bdec22d1b043  City_61->City_19  A230153   \n",
       "4  0157f3db-2a5b-4738-865a-bdec22d1b043  City_61->City_19  A230153   \n",
       "\n",
       "  Material_ID                                   Item_ID Danger_Type   Source  \\\n",
       "0      B-6298  P01-70d4c91f-65e7-434c-89dc-5653fa3d4dff      type_1  City_61   \n",
       "1      B-6298  P01-291a1abe-8b52-4fa9-9667-f6aaba4238bb      type_1  City_61   \n",
       "2      B-6298  P01-1dc8de6a-7ad4-4a3b-a5aa-4b536349f3d7      type_1  City_61   \n",
       "3      B-6298  P01-e1c4511f-f1fc-4f89-b3bd-dea13841510c      type_1  City_61   \n",
       "4      B-6298  P01-86e9b1e3-8f73-49f0-9573-a52a92bbd0f6      type_1  City_61   \n",
       "\n",
       "  Destination           Start_Time         Arrival_Time             Deadline  \\\n",
       "0     City_19  2022-04-05 23:59:59  2022-04-07 01:21:40  2022-04-07 11:59:59   \n",
       "1     City_19  2022-04-05 23:59:59  2022-04-07 01:21:40  2022-04-07 11:59:59   \n",
       "2     City_19  2022-04-05 23:59:59  2022-04-07 01:21:40  2022-04-07 11:59:59   \n",
       "3     City_19  2022-04-05 23:59:59  2022-04-07 01:21:40  2022-04-07 11:59:59   \n",
       "4     City_19  2022-04-05 23:59:59  2022-04-07 01:21:40  2022-04-07 11:59:59   \n",
       "\n",
       "  Shared_Truck  Truck_Type  Area_Rate  Weight_Rate  Capacity_Rate  \n",
       "0            N        16.5   0.880099     0.954333       0.954333  \n",
       "1            N        16.5   0.880099     0.954333       0.954333  \n",
       "2            N        16.5   0.880099     0.954333       0.954333  \n",
       "3            N        16.5   0.880099     0.954333       0.954333  \n",
       "4            N        16.5   0.880099     0.954333       0.954333  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the distance file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the order large file as a pandas dataframe\n",
    "# import pandas as pd\n",
    "\n",
    "# order_path = '../sample_data/order_large.csv'\n",
    "# order_large = pd.read_csv(order_path)\n",
    "\n",
    "# # trim the 0 after second decimal point of Dealine\n",
    "# order_large['Deadline'] = order_large['Deadline'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# # overwrite order large file\n",
    "# order_large.to_csv('../sample_data/order_large.csv', index=False)\n",
    "\n",
    "# # print(len(order_large))\n",
    "# # # read the distance file as a pandas dataframe\n",
    "\n",
    "# # distance = pd.read_csv(distances_path)\n",
    "\n",
    "# # # join order_large and distance to get the distance between Source and Destination\n",
    "\n",
    "# # order_large = order_large.merge(distance, left_on=['Source', 'Destination'], right_on=['Source', 'Destination'])\n",
    "\n",
    "# # print(len(order_large))\n",
    "\n",
    "# # # calculate the time difference between available time and deadline, need to covert the time to datetime object first\n",
    "# # order_large['Available_Time'] = pd.to_datetime(order_large['Available_Time'])\n",
    "# # order_large['Deadline'] = pd.to_datetime(order_large['Deadline'])\n",
    "\n",
    "# # order_large['Time_Diff'] = (order_large['Deadline'] - order_large['Available_Time']).dt.total_seconds()\n",
    "\n",
    "# # # assume the truck speed is 40/3.6 m/s\n",
    "# # truck_speed = 40/3.6\n",
    "\n",
    "# # # calculate the delivery time \n",
    "# # order_large['Delivery_Time'] = order_large['Distance(M)']/truck_speed\n",
    "\n",
    "# # # find the time difference between delivery time and deadline\n",
    "# # order_large['Time_Diff_Delivery'] = order_large['Time_Diff'] - order_large['Delivery_Time']\n",
    "\n",
    "# # # find all orders with Time_Diff_Delivery < 0\n",
    "# # print(len(order_large[order_large['Time_Diff_Delivery'] < 0]))\n",
    "\n",
    "# # # # calculate how many days we need to deliver the order\n",
    "# # # order_large['Delivery_Days'] = order_large['Delivery_Time']/(24*3600)+2\n",
    "\n",
    "\n",
    "\n",
    "# # # # if Time_Diff_Delivery < 0, set the dealine to be Available_Time + Delivery_Days\n",
    "# # # order_large['Deadline'] = order_large.apply(lambda x: x['Available_Time'] + pd.Timedelta(days=x['Delivery_Days']) if x['Time_Diff_Delivery'] < 0 else x['Deadline'], axis=1)\n",
    "\n",
    "# # # order_large['Time_Diff'] = (order_large['Deadline'] - order_large['Available_Time']).dt.total_seconds()\n",
    "# # # order_large['Time_Diff_Delivery'] = order_large['Time_Diff'] - order_large['Delivery_Time']\n",
    "\n",
    "# # # print(len(order_large))\n",
    "\n",
    "# # # # find all orders with Time_Diff_Delivery < 0\n",
    "# # # print(len(order_large[order_large['Time_Diff_Delivery'] < 0]))\n",
    "\n",
    "# # # # Update the order large file with the new deadline, remove all the new added columns\n",
    "# # # order_large = order_large.drop(columns=['Distance(M)','Time_Diff', 'Delivery_Time', 'Time_Diff_Delivery', 'Delivery_Days'])\n",
    "\n",
    "# # # order_large.to_csv('../sample_data/order_large.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('../sample_data/region.json', 'r') as file:\n",
    "#     data = file.read()\n",
    "#     region = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities = []\n",
    "\n",
    "# for province in region['districts']:\n",
    "#     for city in province['districts']:\n",
    "#         if city['level'] == 'city':\n",
    "#             print(city)\n",
    "#             cities.append({\n",
    "#                 'name': city['name'],\n",
    "#                 'longitude': city['center']['longitude'],\n",
    "#                 'latitude': city['center']['latitude']\n",
    "#             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random_numbers = random.sample(range(1, 369), 61)\n",
    "# print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities_mapping = {}\n",
    "# for i in range(0, len(random_numbers)):\n",
    "#     cities_mapping[f'City_{i}'] = cities[random_numbers[i]]\n",
    "\n",
    "# cities_mapping['City_61'] = cities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities_mapping\n",
    "\n",
    "# # convert the citiies_mapping to dataframe, keep only the name, longitude and latitude as the columns header\n",
    "# df = pd.DataFrame(cities_mapping).T.reset_index()\n",
    "\n",
    "# # remove the index column\n",
    "# df = df.drop(columns=['index'])\n",
    "\n",
    "# # save the dataframe to csv\n",
    "\n",
    "# df.to_csv('../sample_data/cities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace the value in source and destination with the city name\n",
    "# distances_path = '../sample_data/distance.csv'\n",
    "# distances = pd.read_csv(distances_path)\n",
    "\n",
    "# # replace the distance value with the distance between the two cities using the latitude and longitude\n",
    "# from geopy.distance import geodesic\n",
    "\n",
    "# distances['Distance(M)'] = distances.apply(lambda x: int(geodesic((cities_mapping[x['Source']]['latitude'], cities_mapping[x['Source']]['longitude']), (cities_mapping[x['Destination']]['latitude'], cities_mapping[x['Destination']]['longitude'])).kilometers*1000), axis=1)\n",
    "\n",
    "# # distance is a panda dataframe object, replace the value in Source and Destination with the city name in dictionary cities_mapping\n",
    "# # for example, if the value in source is 1, replace it with the city name in cities_mapping['City_1']['name']\n",
    "\n",
    "# distances['Source'] = distances['Source'].apply(lambda x: cities_mapping[f'{x}']['name'])\n",
    "# distances['Destination'] = distances['Destination'].apply(lambda x: cities_mapping[f'{x}']['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # overwrite the distance file with the new distance value\n",
    "\n",
    "# distances.to_csv('../sample_data/distance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the order small and order large file\n",
    "# order_small = pd.read_csv('../sample_data/order_small.csv')\n",
    "# order_large = pd.read_csv('../sample_data/order_large.csv')\n",
    "\n",
    "# # replace the value in source and destination with the city name\n",
    "# order_small['Source'] = order_small['Source'].apply(lambda x: cities_mapping[f'{x}']['name'])\n",
    "# order_small['Destination'] = order_small['Destination'].apply(lambda x: cities_mapping[f'{x}']['name'])\n",
    "\n",
    "# order_large['Source'] = order_large['Source'].apply(lambda x: cities_mapping[f'{x}']['name'])\n",
    "# order_large['Destination'] = order_large['Destination'].apply(lambda x: cities_mapping[f'{x}']['name'])\n",
    "\n",
    "# # overwrite the order small and order large file with the new city name\n",
    "# order_small.to_csv('../sample_data/order_small.csv', index=False)\n",
    "# order_large.to_csv('../sample_data/order_large.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimization-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
